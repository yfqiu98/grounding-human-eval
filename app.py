import os
import json
import random
import streamlit as st
import pandas as pd
from datetime import datetime

import base64
import requests

def upload_to_github(file_path, commit_message):
    token = st.secrets["github"]["token"]
    repo = st.secrets["github"]["repo"]
    api_url = f"https://api.github.com/repos/{repo}/contents/{file_path}"

    with open(file_path, "rb") as f:
        content = base64.b64encode(f.read()).decode()

    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }

    data = {
        "message": commit_message,
        "content": content
    }

    # check if file already exists
    r = requests.get(api_url, headers=headers)
    if r.status_code == 200:
        sha = r.json()["sha"]
        data["sha"] = sha

    r = requests.put(api_url, headers=headers, json=data)
    if r.status_code in [200, 201]:
        st.success("✅ File successfully uploaded to GitHub.")
    else:
        st.error(f"❌ GitHub upload failed: {r.status_code} {r.text}")

# ========== CONFIG ==========
MODELS = ["pixinstruct", "got", "chameleon-sft", "chameleon-unsup-sft", "SmartEdit-7B"]
OUTPUT_DIR = "outputs"
# EVAL_INDICES = [i for i in range(21,30)] + [i for i in range(71,80)] + [i for i in range(121,130)] + [i for i in range(221,230)] + [i for i in range(371,380)]
EVAL_INDICES = [0, 50, 100, 200]

TEST_JSON = "test.json"
OUTPUT_PATH = "results"

# ========== LOAD DATA ==========
with open(TEST_JSON, "r") as f:
    test_data = json.load(f)

# ========== SESSION STATE INIT ==========
if "user_id" not in st.session_state:
    st.session_state.user_id = ""
if "annotations" not in st.session_state:
    st.session_state.annotations = []
if "index" not in st.session_state:
    st.session_state.index = 0
if "shuffle_orders" not in st.session_state:
    st.session_state.shuffle_orders = {}

# ========== UI: USER ID ==========
st.title("Anonymous Human Evaluation for Image Editing Models")
st.session_state.user_id = st.text_input("Enter your user ID:", value=st.session_state.user_id)

if not st.session_state.user_id:
    st.warning("Please enter a user ID to begin.")
    st.stop()
    
# ========== INTRODUCTION PAGE ==========
if "intro_shown" not in st.session_state:
    st.session_state.intro_shown = False

if not st.session_state.intro_shown:
    st.header("📝 Evaluation Instructions")

    st.markdown("""
    Welcome, and thank you for participating in this human evaluation study!

    In this task, you will see **image editing results** generated by different anonymous models.
    For each example, your job is to choose:

    - ✅ **The best** output — the one that best follows the instruction, is realistic, and not over-edited.
    - ❌ **The worst** output — the one that least satisfies the instruction or looks least natural.

    **TLDR: in any case, you should consider yourself as an user of a generative image editing system, and give your judgement based on your overall preference.**
    
    You should evaluate the systems according to three criterias,

    ---

    ### 🔧 Criteria 1: Instruction Followed
    - **Good**: The edit reflects the instruction clearly (e.g., "add a tree" results in a tree in the scene).
    - **Bad**: The edit misses the point or changes something irrelevant.

    ### 🎭 Criteria 2: Realism
    - **Good**: The image looks like a real photo with natural textures and lighting.
    - **Bad**: Artifacts, distortions, or uncanny results.

    ### 🎨 Criteria 3: Over-editing
    - **Good**: The edit is focused and minimal, only changing what’s asked.
    - **Bad**: The whole image looks unnaturally altered or cluttered.

    ---
    """)
    st.markdown("### Example instruction: make her turn left")
    st.image("sample_images/input.png", use_container_width=True)
    
    col0, col1, col2, col3 = st.columns(4)
    with col0:
        st.image("sample_images/positive.png", caption="✅ Follows instruction well, realistic, and not over-edited)", use_container_width=True)
    with col1:
        st.image("sample_images/negative-donotedit.png", caption="❌ Instruction Not Followed\n(e.g., model simply copies the input as output or fails to follow the instruction.)", use_container_width=True)
    with col2:
        st.image("sample_images/negative-overedit.png", caption="❌ Over-edited\n(e.g., too many changes, model add an additional people into the scene.)", use_container_width=True)
    with col3:
        st.image("sample_images/negative-unrealistic.png", caption="❌ Unrealistic\n(e.g., distorted lighting and model fails to produce the output according to the input.)", use_container_width=True)

    st.markdown("""
    ---
    When you're ready, click below to start evaluating. You can always return to a previous sample to modify your response.
    """)


    if st.button("Start Evaluation"):
        st.session_state.intro_shown = True
        st.rerun()

    st.stop()

# ========== EVALUATION FLOW ==========
i = st.session_state.index
if i >= len(EVAL_INDICES):
    st.success("You have completed all evaluations. Thank you!")
    df = pd.DataFrame(st.session_state.annotations)
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    filename = f"{OUTPUT_PATH}/annotations_{st.session_state.user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(filename, index=False)
    st.write("Your annotations have been saved as:", filename)
    st.stop()

# ========== DISPLAY SAMPLE ==========
sample_index = EVAL_INDICES[i]
sample = test_data[sample_index]

# Shuffle display order but store it to ensure correct mapping
if sample_index not in st.session_state.shuffle_orders:
    shuffled_models = MODELS.copy()
    random.shuffle(shuffled_models)
    st.session_state.shuffle_orders[sample_index] = shuffled_models
else:
    shuffled_models = st.session_state.shuffle_orders[sample_index]

# ========== DISPLAY ==========
st.markdown(f"#### Image ID: `{sample_index}`")
st.markdown(f"<h3 style='color:#333'>Instruction for Editing: {sample['instruction']}</h3>", unsafe_allow_html=True)

st.markdown("**Input Image**")
st.image(sample['input'], width=300)

st.markdown("#### Anonymous Model Outputs")
image_filename = f"{sample_index}.png"
model_display_info = []

for idx, model in enumerate(shuffled_models):
    tag = f"Model {idx+1}"
    img_path = os.path.join(OUTPUT_DIR, model, image_filename)
    model_display_info.append((tag, model, img_path))

output_cols = st.columns(len(model_display_info))
for col, (tag, _, img_path) in zip(output_cols, model_display_info):
    with col:
        st.image(img_path, caption=tag, use_container_width=True)

# ========== ANNOTATION FORM ==========
st.markdown("### Evaluate each model on a 1–5 scale (1 = Poor, 5 = Excellent)")

def likert_input(question, sample_index):
    st.markdown(f"**{question}**")
    scores = {}
    for tag, model, _ in model_display_info:
        # include sample_index in the key
        widget_key = f"{question}-{tag}-sample{sample_index}"
        score = st.selectbox(
            tag,
            [1, 2, 3, 4, 5],
            index=2,           # default to 3
            key=widget_key
        )
        scores[model] = score
    return scores

def best_worst_input(question, sample_index):
    st.markdown(f"**{question}**")

    # Display tags (anonymous model labels)
    tags = [tag for tag, _, _ in model_display_info]
    tag_to_model = {tag: model for tag, model, _ in model_display_info}

    # best_options = ["None of them is the best"] + tags
    # worst_options = ["None of them is the worst"] + tags
    best_options = tags
    worst_options = tags

    best_key = f"{question}-best-sample{sample_index}"
    worst_key = f"{question}-worst-sample{sample_index}"

    best = st.radio(f"Select the BEST model:", best_options, index=0, key=best_key, horizontal=True)
    worst = st.radio(f"Select the WORST model:", worst_options, index=0, key=worst_key, horizontal=True)

    scores = {}
    for tag, model, _ in model_display_info:
        if best == "None of them is the best" and worst == "None of them is the worst":
            scores[model] = 0
        elif tag == best:
            scores[model] = 1
        elif tag == worst:
            scores[model] = -1
        else:
            scores[model] = 0
    return scores

# edit_scores    = likert_input("1. Did the model correctly perform the editing?", sample_index)
# over_scores    = likert_input("2. Is the image over-edited?",               sample_index)
# realism_scores = likert_input("3. How realistic is the result?",           sample_index)

edit_scores    = best_worst_input("Select the ", sample_index)
# over_scores    = best_worst_input("2. Is the image over-edited?",               sample_index)
# realism_scores = best_worst_input("3. How realistic is the result?",           sample_index)

# ========== SUBMIT ==========
if st.button("Submit Evaluation"):
    for model in MODELS:
        record = {
            "user_id": st.session_state.user_id,
            "sample_index": sample_index,
            "model": model,
            "edit_score": edit_scores[model],
            # "overedit_score": over_scores[model],
            # "realism_score": realism_scores[model],
        }
        st.session_state.annotations.append(record)

    st.session_state.index += 1
    st.experimental_set_query_params()
    st.rerun()

# ========== RETURN TO PREVIOUS ==========
if i >= len(EVAL_INDICES):
    st.success("You have completed all evaluations. Thank you!")
    df = pd.DataFrame(st.session_state.annotations)
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    filename = f"{OUTPUT_PATH}/annotations_{st.session_state.user_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(filename, index=False)

    # Upload to GitHub
    remote_filename = f"results/{os.path.basename(filename)}"
    commit_message = f"Add annotation file: {remote_filename}"
    upload_to_github(remote_filename, commit_message)

    st.write("Your annotations have been saved locally and pushed to GitHub:")
    st.code(remote_filename)
    st.stop()